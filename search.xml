<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【每日早报】</title>
      <link href="/2025/11/03/%E3%80%90%E6%AF%8F%E6%97%A5%E6%97%A9%E6%8A%A5%E3%80%91/"/>
      <url>/2025/11/03/%E3%80%90%E6%AF%8F%E6%97%A5%E6%97%A9%E6%8A%A5%E3%80%91/</url>
      
        <content type="html"><![CDATA[<!DOCTYPE html><html lang="en"><head>    <meta charset="UTF-8">    <meta http-equiv="X-UA-Compatible" content="IE=edge">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>每日早报</title></head><body>    <div style="text-align: center;"> <img src="https://file.alapi.cn/60s/202511031762110902.png"            alt="每日早报" width="100%"> </div></body></html>]]></content>
      
      
      <categories>
          
          <category> 每日早报 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日早报 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>修改键盘F9为Home键</title>
      <link href="/2025/10/22/changekey/"/>
      <url>/2025/10/22/changekey/</url>
      
        <content type="html"><![CDATA[<h2 id="修改键盘F9键功能-将其映射为HOME键"><a href="#修改键盘F9键功能-将其映射为HOME键" class="headerlink" title="修改键盘F9键功能,将其映射为HOME键"></a>修改键盘F9键功能,将其映射为HOME键</h2><blockquote><p>由于键盘是75键, 本身不具备HOME键, 但是日常和工作对HOME键还是挺有需要的(别问为什么不买全键的, 个人不太喜欢数字区, 对75键构造独钟)</p></blockquote><p>回归正题, 如何实现</p><p>我采用的是修改注册表方式</p><p>网上能搜到比较多方案, 除了注册表, 一般都需要下载额外软件, 键盘驱动或者是windows软件来将按键功能映射为其他</p><p>所以, 修改注册表, 省时省力</p><h3 id="修改注册表"><a href="#修改注册表" class="headerlink" title="修改注册表"></a>修改注册表</h3><p>1.打开注册表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按下键盘上 win + r 并输入</span></span><br><span class="line">regidit</span><br></pre></td></tr></table></figure><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/regidit.png"></p><p>2.定位到按键修改路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Keyboard Layout</span><br></pre></td></tr></table></figure><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/inregidit.png"></p><p>3.在右侧区域新建二进制值 -&gt; 命名为 Scancode Map, 并填入下列值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">02 00 00 00 47 E0 43 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br></pre></td></tr></table></figure><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/binaryvalue.png"></p><p>4.点击确定, 退出注册表, 重启电脑, 永久生效👍</p>]]></content>
      
      
      <categories>
          
          <category> 效率 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 方法 </tag>
            
            <tag> 技巧 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>反编译 APK</title>
      <link href="/2025/03/27/decodeapk/"/>
      <url>/2025/03/27/decodeapk/</url>
      
        <content type="html"><![CDATA[<h1 id="Android逆向"><a href="#Android逆向" class="headerlink" title="Android逆向"></a>Android逆向</h1><h2 id="反编译APK"><a href="#反编译APK" class="headerlink" title="反编译APK"></a>反编译APK</h2><h3 id="工具介绍"><a href="#工具介绍" class="headerlink" title="工具介绍"></a>工具介绍</h3><p>如果只是想拿到apk中的图片资源，只需要将apk后缀改为zip然后解压缩，<strong>res</strong>目录中就包含了所有的资源文件</p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/releasezip.png"></p><p><strong>classes.dex</strong> 则包含了所有的代码，只是还无法查看</p><p><strong>AndroidManifest.xml</strong> 文件打开会发现无法阅读，都是16进制数</p><p>此时就需要用到工具 —— ApkTool</p><h4 id="ApkTool"><a href="#ApkTool" class="headerlink" title="ApkTool"></a>ApkTool</h4><h5 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h5><p><a href="https://apktool.org/">ApkTool官网</a></p><h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/apktooldocs.png"></p><h5 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apktool d xxx.apk</span><br></pre></td></tr></table></figure><blockquote><p>d 表示 decode</p><p>还可以加上一些附加参数来控制 decode 行为：</p><p>-f ：如果目标文件夹已存在，则强制删除现有文件夹（默认如果目标文件夹已存在，则解码失败）</p><p>-o ：指定解码目标文件夹的名称（默认使用 APK 文件的名字来命名目标文件夹）</p><p>-s ：不反编译dex文件，也就是说 classes.dex 文件会被保留（默认会将 dex 文件解码成 smali 文件）</p><p>-r ：不反编译资源文件，也就是说 resources.arsc 文件会被保留（默认会将 resources.arsc 解码成具体的资源文件）</p></blockquote><p>反编译之后会得到以下内容：</p><ul><li>1、AndroidManifest.xml：经过反编译还原后的 manifest 文件</li><li>2、original 文件夹：存放了未经反编译过、原始的 AndroidManifest.xml 文件</li><li>3、res 文件夹：存放了反编译出来的所有资源</li><li>4、smali 文件夹：存放了反编译出来的所有代码，只不过格式都是<code>.smali</code>类型的</li></ul><p>xml文件已经可以看懂了，不过 smali 类型文件我们依然无法阅读</p><p>此时，需要用到另一个工具 —— dex2jar + jd-gui</p><h4 id="dex2jar"><a href="#dex2jar" class="headerlink" title="dex2jar"></a>dex2jar</h4><h5 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h5><p>将 dex 转换成 jar 形式文件</p><h5 id="下载-1"><a href="#下载-1" class="headerlink" title="下载"></a>下载</h5><p><a href="https://sourceforge.net/projects/dex2jar/files/">dex2jar官网</a></p><h5 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h5><p>将下载的 dex2jar 压缩包解压后，可以看到以下内容</p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/dex2jar.png"></p><p>windows上使用dex2jar.bat即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dex2jar.bat classes.dex路径</span><br></pre></td></tr></table></figure><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/dexcontent.png"></p><p>看到上述console则表示成功</p><p>代码都位于 classes-dex2jar.jar 中</p><p>现在需要用到另一款工具 jd-gui</p><h4 id="jd-gui"><a href="#jd-gui" class="headerlink" title="jd-gui"></a>jd-gui</h4><h5 id="下载-2"><a href="#下载-2" class="headerlink" title="下载"></a>下载</h5><p><a href="https://java-decompiler.github.io/">jd-gui官网</a></p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/jdgui.png"></p><p>根据需要下载对应包即可</p><h5 id="使用-2"><a href="#使用-2" class="headerlink" title="使用"></a>使用</h5><p><img src="C:\Users\bamboo\AppData\Roaming\Typora\typora-user-images\image-20250327145549112.png" alt="image-20250327145549112"></p><p>解压到本地，双击jd-gui.exe文件即可运行</p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/jarcontent.png"></p><p>用jd-gui打开之前解压出来的dex文件即可看到所有的源码</p><h4 id="jadx-gui"><a href="#jadx-gui" class="headerlink" title="jadx-gui"></a>jadx-gui</h4><blockquote><p>一个更强大的工具，一款出色的 **反编译工具 **和 <strong>代码查看器</strong>，但不能直接编辑 APK 文件或内部代码</p><p>使用 Jadx-GUI 打开一个apk文件时，它会根据 Dalvik 字节码（DEX文件）反编译成可读的 Java 源代码，然而，这些源代码只是 Jadx 根据字节码猜测出来的，并不是原始的、可变翼德Java源文件，因此，无法直接在 Jadx-GUI 中修改这些反编译出来的 Java 代码。</p></blockquote><p><strong>下载</strong></p><p><a href="https://github.com/skylot/jadx">Jadx-GUI</a></p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/jadxgui.png"></p><p>使用起来也很简单，打开exe文件</p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/jadxuse.png"></p><p>然后点击打开文件&#x2F;打开项目或者将apk文件直接拖拽过来即可查看；</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
          <category> 逆向工程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decode </tag>
            
            <tag> 安卓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>有用的小知识</title>
      <link href="/2025/03/01/justrecord/"/>
      <url>/2025/03/01/justrecord/</url>
      
        <content type="html"><![CDATA[<h2 id="银屑病一些好用的药物"><a href="#银屑病一些好用的药物" class="headerlink" title="银屑病一些好用的药物"></a>银屑病一些好用的药物</h2><p>他克莫司</p><h2 id="补钙"><a href="#补钙" class="headerlink" title="补钙"></a>补钙</h2><p>人体越越钙，吸收率越高，一般20% ~ 30%，缺钙的话可能提高到60 ~ 70，不缺钙可能会下降到10%左右（这是在正常摄入钙的情况下）</p><p>柠檬酸钙（较贵）<br>碳酸钙（同样可以）<br>每天补充 700mg 以上</p><p>补充氨基酸可以促进钙的吸收</p><p>所以，有复合产品 —— 氨基酸螯合钙等</p><p>钙补充多了会便秘（吸收率有限），多做运动保证吸收的钙能有效利用</p><p>多补充维生素D！！！</p><h2 id="最让人舒服的11种颜色RGB值和十六进制值"><a href="#最让人舒服的11种颜色RGB值和十六进制值" class="headerlink" title="最让人舒服的11种颜色RGB值和十六进制值"></a>最让人舒服的11种颜色RGB值和十六进制值</h2><table><thead><tr><th>序号</th><th align="left">名称</th><th align="left">RGB</th><th align="left">十六进制</th></tr></thead><tbody><tr><td>1</td><td align="left">豆沙绿</td><td align="left">(199, 237, 204)</td><td align="left">#C7EDCC</td></tr><tr><td>2</td><td align="left">银河白</td><td align="left">(255, 255, 255)</td><td align="left">#FFFFFF</td></tr><tr><td>3</td><td align="left">杏仁黄</td><td align="left">(250, 249, 222)</td><td align="left">#FAF9DE</td></tr><tr><td>4</td><td align="left">秋叶褐</td><td align="left">(255, 242, 226)</td><td align="left">#FFF2E2</td></tr><tr><td>5</td><td align="left">胭脂红</td><td align="left">(253, 230, 224)</td><td align="left">#FDE6E0</td></tr><tr><td>6</td><td align="left">青草绿</td><td align="left">(227, 237, 205)</td><td align="left">#E3EDCD</td></tr><tr><td>7</td><td align="left">海天蓝</td><td align="left">(220, 226, 241)</td><td align="left">#DCE2F1</td></tr><tr><td>8</td><td align="left">葛巾紫</td><td align="left">(233, 235, 254)</td><td align="left">#E9EBFE</td></tr><tr><td>9</td><td align="left">极光灰</td><td align="left">(234, 234, 239)</td><td align="left">#EAEAEF</td></tr><tr><td>10</td><td align="left">苹果绿</td><td align="left">(183, 232, 189)</td><td align="left">#B7E8BD</td></tr><tr><td>11</td><td align="left">豆沙绿-略暗</td><td align="left">(204, 232, 207)</td><td align="left">#CCE8CF</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随手一记 </tag>
            
            <tag> 健康 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优秀文章 - Android</title>
      <link href="/2025/01/06/nicearticles/"/>
      <url>/2025/01/06/nicearticles/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> 文章 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小技巧</title>
      <link href="/2024/12/21/someskills/"/>
      <url>/2024/12/21/someskills/</url>
      
        <content type="html"><![CDATA[<h2 id="如何一次性删除所有以某关键词结尾的文件"><a href="#如何一次性删除所有以某关键词结尾的文件" class="headerlink" title="如何一次性删除所有以某关键词结尾的文件"></a>如何一次性删除所有以某关键词结尾的文件</h2><blockquote><p>其他类型删除以此类推</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find . -type f -name &#x27;*.ko&#x27; -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure><h2 id="Git-push报错"><a href="#Git-push报错" class="headerlink" title="Git push报错"></a>Git push报错</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh: connect to host github.com port 22: Connection refused fatal: Could not read from remote repository.  Please make sure you have the correct access rights and the repository exists.</span><br></pre></td></tr></table></figure><p>这个错误表明 Git 无法通过 SSH（端口 22）连接到 GitHub。可能的原因和解决方法如下：</p><h3 id="1-检查-SSH-连接"><a href="#1-检查-SSH-连接" class="headerlink" title="1. 检查 SSH 连接"></a>1. <strong>检查 SSH 连接</strong></h3><p>运行以下命令测试 SSH 连接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>如果连接被拒绝，可能是网络或防火墙问题。</p><h3 id="2-改用-HTTPS-协议"><a href="#2-改用-HTTPS-协议" class="headerlink" title="2. 改用 HTTPS 协议"></a>2. <strong>改用 HTTPS 协议</strong></h3><p>如果 SSH 被屏蔽，可以临时改用 HTTPS：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote set-url origin https://github.com/用户名/仓库名.git</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p>（需要输入 GitHub 账号密码或个人访问令牌）</p><h3 id="3-尝试-SSH-端口-443"><a href="#3-尝试-SSH-端口-443" class="headerlink" title="3. 尝试 SSH 端口 443"></a>3. <strong>尝试 SSH 端口 443</strong></h3><p>如果 22 端口被屏蔽，GitHub 也支持通过 443 端口使用 SSH。编辑 <code>~/.ssh/config</code> 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Host github.com</span><br><span class="line">User xxxxqq.com</span><br><span class="line">Hostname ssh.github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa</span><br><span class="line">Port 443</span><br></pre></td></tr></table></figure><p>然后再次测试 SSH 连接。</p><h3 id="4-检查防火墙-代理设置"><a href="#4-检查防火墙-代理设置" class="headerlink" title="4. 检查防火墙&#x2F;代理设置"></a>4. <strong>检查防火墙&#x2F;代理设置</strong></h3><ul><li>确保本地防火墙或公司网络未屏蔽 SSH（端口 22&#x2F;443）</li><li>如果使用代理，需配置 Git 使用代理：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.proxy http://代理地址:端口</span><br><span class="line">git config --global https.proxy https://代理地址:端口</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-验证-SSH-密钥"><a href="#5-验证-SSH-密钥" class="headerlink" title="5. 验证 SSH 密钥"></a>5. <strong>验证 SSH 密钥</strong></h3><p>确保你的 SSH 密钥已添加到 GitHub：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>然后将内容粘贴到 GitHub Settings → SSH and GPG keys。</p><h3 id="6-检查仓库是否存在"><a href="#6-检查仓库是否存在" class="headerlink" title="6. 检查仓库是否存在"></a>6. <strong>检查仓库是否存在</strong></h3><p>确认远程仓库地址正确且存在：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 效率 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
            <tag> 命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>张量索引切片操作</title>
      <link href="/2024/12/21/%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95%E5%88%87%E7%89%87%E6%93%8D%E4%BD%9C/"/>
      <url>/2024/12/21/%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95%E5%88%87%E7%89%87%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>大致按照使用频率递减给出</p></blockquote><blockquote><p><strong>这里可以给出一个结论性的规律, 便于判断张量形状, 索引操作时, 有几个 : , 就有几个维度</strong></p></blockquote><h1 id="1-基本下标与切片（Python-风格）"><a href="#1-基本下标与切片（Python-风格）" class="headerlink" title="1. 基本下标与切片（Python 风格）"></a>1. 基本下标与切片（Python 风格）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)  <span class="comment"># shape (2,3,4)</span></span><br><span class="line"><span class="comment"># x[batch, row, col]</span></span><br><span class="line">x[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]        <span class="comment"># 标量 tensor(0)</span></span><br><span class="line">x[<span class="number">0</span>, :, :]      <span class="comment"># 第一维=0，取出 shape (3,4)</span></span><br><span class="line">x[:, <span class="number">1</span>, :]      <span class="comment"># 所有batch, 第二个row -&gt; shape (2,4)</span></span><br><span class="line">x[..., <span class="number">2</span>]       <span class="comment"># 省略号，等价于 x[:, :, 2] -&gt; shape (2,3)</span></span><br><span class="line">x[<span class="number">0</span>]            <span class="comment"># 等价于 x[0, :, :] -&gt; shape (3,4)</span></span><br></pre></td></tr></table></figure><ul><li>切片语法支持 <code>start:stop:step</code>（含 start，不含 stop），支持负数索引和步长。</li><li>例如 <code>x[:, ::-1, :]</code> 会在中间维度上反转顺序（返回 view 还是 copy 取决于实现；在 PyTorch 中 negative step 会返回 copy）。</li></ul><h1 id="2-使用-None-np-newaxis（增加维度）"><a href="#2-使用-None-np-newaxis（增加维度）" class="headerlink" title="2. 使用 None &#x2F; np.newaxis（增加维度）"></a>2. 使用 <code>None</code> &#x2F; <code>np.newaxis</code>（增加维度）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])        <span class="comment"># shape (3,)</span></span><br><span class="line">y[<span class="literal">None</span>, :]      <span class="comment"># shape (1,3)</span></span><br><span class="line">y[:, <span class="literal">None</span>]      <span class="comment"># shape (3,1)</span></span><br></pre></td></tr></table></figure><ul><li>常用于把向量转为列&#x2F;行以便广播。</li></ul><h1 id="3-布尔掩码（Boolean-Masking）"><a href="#3-布尔掩码（Boolean-Masking）" class="headerlink" title="3. 布尔掩码（Boolean Masking）"></a>3. 布尔掩码（Boolean Masking）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">3</span>])</span><br><span class="line">mask = a &gt; <span class="number">3</span>                 <span class="comment"># tensor([False, True, False, True, False])</span></span><br><span class="line">a[mask]                     <span class="comment"># tensor([5,7])  -&gt; 1D 输出，丢失原shape信息</span></span><br></pre></td></tr></table></figure><ul><li><code>masked_select(a, mask)</code> 等价于 <code>a[mask]</code>。</li><li>当 mask 是多维且与 a 同 shape 时，结果是扁平的 1D 张量（按行主序提取元素）。</li><li>可用于筛样本、实现 padding 掩码筛选等。</li></ul><h1 id="4-花式索引（整数数组索引-Advanced-Indexing）"><a href="#4-花式索引（整数数组索引-Advanced-Indexing）" class="headerlink" title="4. 花式索引（整数数组索引 &#x2F; Advanced Indexing）"></a>4. 花式索引（整数数组索引 &#x2F; Advanced Indexing）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">M = torch.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)  <span class="comment"># shape (3,4)</span></span><br><span class="line">rows = torch.tensor([<span class="number">2</span>,<span class="number">0</span>])</span><br><span class="line">cols = torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">M[rows]           <span class="comment"># 按行选择 -&gt; shape (2,4)</span></span><br><span class="line">M[:, cols]        <span class="comment"># 按列选择 -&gt; shape (3,2)</span></span><br><span class="line"><span class="comment"># 对应元素选择（pairwise）</span></span><br><span class="line">M[rows, cols]     <span class="comment"># 取 (2,1) 和 (0,3) -&gt; shape (2,)</span></span><br></pre></td></tr></table></figure><ul><li>若使用多个 1D 整数索引（同维度），会进行<strong>逐元素配对</strong>索引，输出长度等于索引数组长度。</li><li>若混合切片和整数数组索引，规则稍复杂：整数索引会先被应用，结果维度位置会消失或变为新维度。</li></ul><p>例（多维）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">T = torch.arange(<span class="number">2</span>*<span class="number">3</span>*<span class="number">4</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">idx0 = torch.tensor([<span class="number">0</span>,<span class="number">1</span>])    <span class="comment"># 用于第0维</span></span><br><span class="line">idx1 = torch.tensor([<span class="number">2</span>,<span class="number">0</span>])    <span class="comment"># 用于第1维</span></span><br><span class="line">T[idx0, idx1]    <span class="comment"># 逐对索引 -&gt; shape (2,4)</span></span><br><span class="line"><span class="comment"># 等价于: torch.stack([T[0,2], T[1,0]], dim=0)</span></span><br></pre></td></tr></table></figure><h1 id="5-广播与索引（注意形状）"><a href="#5-广播与索引（注意形状）" class="headerlink" title="5. 广播与索引（注意形状）"></a>5. 广播与索引（注意形状）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)  <span class="comment"># (2,3)</span></span><br><span class="line">idx = torch.tensor([<span class="number">0</span>,<span class="number">2</span>])         <span class="comment"># (2,)</span></span><br><span class="line">A[torch.arange(<span class="number">2</span>), idx]           <span class="comment"># (2,) -&gt; 每 batch 对应列取值</span></span><br><span class="line"><span class="comment"># torch.arange(2) 为 [0,1]，与 idx 配对 -&gt; 取 (0,0) 和 (1,2)</span></span><br></pre></td></tr></table></figure><ul><li>常用于按-batch 选择每个样本对应的索引（如分类预测的 top-k 判断）。</li></ul><h1 id="6-gather-与-scatter（按索引收集-写入，适用于高维批量操作）"><a href="#6-gather-与-scatter（按索引收集-写入，适用于高维批量操作）" class="headerlink" title="6. gather 与 scatter（按索引收集&#x2F;写入，适用于高维批量操作）"></a>6. <code>gather</code> 与 <code>scatter</code>（按索引收集&#x2F;写入，适用于高维批量操作）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gather 示例：从 src 中按 index 收集（需要指定 dim）</span></span><br><span class="line">src = torch.tensor([[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>],[<span class="number">20</span>,<span class="number">21</span>,<span class="number">22</span>]])   <span class="comment"># shape (2,3)</span></span><br><span class="line">index = torch.tensor([[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]])       <span class="comment"># shape (2,3)</span></span><br><span class="line">torch.gather(src, dim=<span class="number">1</span>, index=index)</span><br><span class="line"><span class="comment"># -&gt; shape (2,3): [[12,11,10],[20,22,21]]</span></span><br></pre></td></tr></table></figure><ul><li><code>gather</code> 要求 <code>index</code> 与 <code>src</code> 在除了 <code>dim</code> 外的维度完全相同；返回与 <code>index</code> 同形状的张量。</li><li>常用于实现按位置取值（例如 beam-search、按预测索引从概率张量中取值）。</li><li><code>scatter_</code>&#x2F;<code>scatter</code> 用于把值写入指定位置，可做 one-hot 化或累积（有 <code>reduce</code> 参数）。</li></ul><h1 id="7-index-select-take（按维度选择）"><a href="#7-index-select-take（按维度选择）" class="headerlink" title="7. index_select &#x2F; take（按维度选择）"></a>7. <code>index_select</code> &#x2F; <code>take</code>（按维度选择）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>])</span><br><span class="line">torch.index_select(v, dim=<span class="number">0</span>, index=torch.tensor([<span class="number">3</span>,<span class="number">1</span>]))  <span class="comment"># tensor([40,20])</span></span><br><span class="line"><span class="comment"># 对于矩阵按行选：</span></span><br><span class="line">M = torch.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">torch.index_select(M, dim=<span class="number">0</span>, index=torch.tensor([<span class="number">2</span>,<span class="number">0</span>]))  <span class="comment"># shape (2,4)</span></span><br></pre></td></tr></table></figure><ul><li><code>index_select</code> 返回的顺序与索引一致；与 fancy indexing（<code>M[idx]</code>）相似，但有些后端实现行为细微不同（比如保留 contiguous 性）。</li></ul><h1 id="8-masked-fill-where（掩码赋值-条件选择）"><a href="#8-masked-fill-where（掩码赋值-条件选择）" class="headerlink" title="8. masked_fill, where（掩码赋值 &#x2F; 条件选择）"></a>8. <code>masked_fill</code>, <code>where</code>（掩码赋值 &#x2F; 条件选择）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.</span>, -<span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">x.masked_fill(x &lt; <span class="number">0</span>, <span class="number">0.</span>)    <span class="comment"># 把负数置0</span></span><br><span class="line">torch.where(x&gt;<span class="number">0</span>, x, torch.zeros_like(x))  <span class="comment"># 条件选择，相当于 np.where</span></span><br></pre></td></tr></table></figure><ul><li><code>where(cond, A, B)</code> 返回与 <code>A</code>&#x2F;<code>B</code> 广播后的形状相同的张量。</li></ul><h1 id="9-unsqueeze-squeeze-与-view-reshape（维度控制）"><a href="#9-unsqueeze-squeeze-与-view-reshape（维度控制）" class="headerlink" title="9. unsqueeze &#x2F; squeeze 与 view/reshape（维度控制）"></a>9. <code>unsqueeze</code> &#x2F; <code>squeeze</code> 与 <code>view/reshape</code>（维度控制）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])      <span class="comment"># (3,)</span></span><br><span class="line">a.unsqueeze(<span class="number">0</span>)                 <span class="comment"># (1,3)</span></span><br><span class="line">a.unsqueeze(<span class="number">1</span>)                 <span class="comment"># (3,1)</span></span><br><span class="line">torch.squeeze(a.unsqueeze(<span class="number">0</span>))  <span class="comment"># 恢复</span></span><br></pre></td></tr></table></figure><ul><li><code>squeeze(dim)</code> 只在指定维度为 1 时删除该维度。</li><li><code>reshape</code>&#x2F;<code>view</code> 会改变内存视图（<code>view</code> 要求连续 contiguous；<code>reshape</code> 在必要时会复制）。</li></ul><h1 id="10-Ellipsis-（省略号）"><a href="#10-Ellipsis-（省略号）" class="headerlink" title="10. Ellipsis ...（省略号）"></a>10. Ellipsis <code>...</code>（省略号）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>)</span><br><span class="line">X[..., <span class="number">0</span>]   <span class="comment"># 等价 X[:, :, :, 0]</span></span><br><span class="line">X[<span class="number">0</span>, ...]   <span class="comment"># 等价 X[0, :, :, :]</span></span><br></pre></td></tr></table></figure><ul><li>在不确定前面&#x2F;后面维度数时非常有用，特别是在写通用层时。</li></ul><h1 id="11-多维返回与维度插入（保持-丢失维度）"><a href="#11-多维返回与维度插入（保持-丢失维度）" class="headerlink" title="11. 多维返回与维度插入（保持&#x2F;丢失维度）"></a>11. 多维返回与维度插入（保持&#x2F;丢失维度）</h1><ul><li>使用整数索引会<strong>减少</strong>维度（那一维被消除）；</li><li>使用切片或 <code>None</code>，或保持长度为 1 的索引会保留维度。<br> 示例：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t[<span class="number">0</span>].shape        <span class="comment"># (3,4)   -- 整数索引去掉第0维</span></span><br><span class="line">t[<span class="number">0</span>:<span class="number">1</span>].shape      <span class="comment"># (1,3,4) -- 切片保留第0维</span></span><br><span class="line">t[[<span class="number">0</span>]].shape      <span class="comment"># (1,3,4) -- 用长度1的索引数组也保留</span></span><br></pre></td></tr></table></figure><h1 id="12-视图（view）与-copy（内存-contiguous）相关注意"><a href="#12-视图（view）与-copy（内存-contiguous）相关注意" class="headerlink" title="12. 视图（view）与 copy（内存&#x2F;contiguous）相关注意"></a>12. 视图（view）与 copy（内存&#x2F;contiguous）相关注意</h1><ul><li>大多数简单切片和整型索引会返回<strong>原张量的 view（共享内存）</strong>，但有些操作会返回 <strong>copy</strong>（例如带负步长的切片、某些高级索引）。</li><li><code>is_contiguous()</code> 可以检查是否连续。若对返回的张量执行 <code>view()</code> 可能会报错，需先 <code>.contiguous()</code>。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">s = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t = s[:, ::-<span class="number">1</span>]           <span class="comment"># 可能是 copy（不连续）</span></span><br><span class="line">t.is_contiguous()       <span class="comment"># 可能 False</span></span><br><span class="line">t.contiguous().view(-<span class="number">1</span>) <span class="comment"># 安全</span></span><br></pre></td></tr></table></figure><ul><li>在 in-place 操作（如 <code>t += 1</code>）时，如果 t 与原张量共享内存，可能会影响原张量；对 copy 则无影响。</li></ul><h1 id="13-反向传播（autograd）相关"><a href="#13-反向传播（autograd）相关" class="headerlink" title="13. 反向传播（autograd）相关"></a>13. 反向传播（autograd）相关</h1><ul><li>索引、切片会保留计算图信息（如果原张量 <code>requires_grad=True</code>），因此从张量中取出的部分仍可对原张量反向传播。</li><li>但是，用高级索引赋值（<code>x[idx] = something</code>）不记录梯度；需要使用 <code>scatter</code> 或构造新的张量再计算 loss。</li><li><code>detach()</code> 可以切断梯度传播（例如 <code>x = x.detach()</code>）。</li></ul><p>示例（反向传播影响）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x[<span class="number">1</span>] * <span class="number">2</span></span><br><span class="line">y.backward()           <span class="comment"># 会为 x[1] 累积梯度，但 x[0], x[2] 为 0</span></span><br><span class="line">x.grad                <span class="comment"># tensor([0., 2., 0.])</span></span><br></pre></td></tr></table></figure><h1 id="14-常见用途与模式（实战片段）"><a href="#14-常见用途与模式（实战片段）" class="headerlink" title="14. 常见用途与模式（实战片段）"></a>14. 常见用途与模式（实战片段）</h1><ul><li><strong>按 batch 取样</strong>（分类概率取预测值）：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">probs = torch.randn(<span class="number">32</span>, <span class="number">10</span>)  <span class="comment"># logits or probs</span></span><br><span class="line">pred = probs.argmax(dim=<span class="number">1</span>)   <span class="comment"># (32,)</span></span><br><span class="line"><span class="comment"># 如果想从 probs 中收集每个 batch 对应预测的概率：</span></span><br><span class="line">selected = probs[torch.arange(<span class="number">32</span>), pred]  <span class="comment"># shape (32,)</span></span><br></pre></td></tr></table></figure><ul><li><strong>padding mask</strong>（seq 长短不一）：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">seq = torch.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">4</span>,<span class="number">7</span>))   <span class="comment"># batch, seq_len</span></span><br><span class="line">mask = (seq != PAD_TOKEN)            <span class="comment"># True 表示有效</span></span><br><span class="line"><span class="comment"># 通过 mask 做池化：</span></span><br><span class="line">masked_sum = (embeddings * mask.unsqueeze(-<span class="number">1</span>)).<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>one-hot</strong>：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">idx = torch.tensor([<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">onehot = torch.nn.functional.one_hot(idx, num_classes=<span class="number">4</span>)  <span class="comment"># shape (3,4)</span></span><br></pre></td></tr></table></figure><ul><li><strong>按索引更新参数</strong>（embedding lookup 与更新）：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">embedding = torch.nn.Embedding(num_embeddings=<span class="number">1000</span>, embedding_dim=<span class="number">64</span>)</span><br><span class="line">out = embedding(idx_tensor)   <span class="comment"># embedding 内部就是高级索引/ gather 实现</span></span><br></pre></td></tr></table></figure><h1 id="15-进阶：einsum-作为灵活替代（当索引和-reshape-太繁琐）"><a href="#15-进阶：einsum-作为灵活替代（当索引和-reshape-太繁琐）" class="headerlink" title="15. 进阶：einsum 作为灵活替代（当索引和 reshape 太繁琐）"></a>15. 进阶：<code>einsum</code> 作为灵活替代（当索引和 reshape 太繁琐）</h1><ul><li><code>einsum</code> 可以在一次表达式中完成复杂的通道&#x2F;维度重新排列与约简，可替代多个 <code>transpose</code> + <code>matmul</code> 操作。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例：批量矩阵乘积 sum over k: c_&#123;ij&#125; = sum_k a_&#123;ik&#125; b_&#123;jk&#125;</span></span><br><span class="line">torch.einsum(<span class="string">&#x27;ik,jk-&gt;ij&#x27;</span>, a, b)</span><br></pre></td></tr></table></figure><h1 id="16-常见陷阱与建议"><a href="#16-常见陷阱与建议" class="headerlink" title="16. 常见陷阱与建议"></a>16. 常见陷阱与建议</h1><ol><li><strong>整数数组索引通常会返回 copy（非 view）</strong> —— 这会影响内存并且后续 in-place 修改不会影响原张量。</li><li><strong>带负步长的切片常常产生 copy</strong>，要注意 <code>is_contiguous()</code>。</li><li><strong>混合使用布尔掩码与维度不适配会报错</strong>，确保 mask 与被掩的张量形状一致或能广播。</li><li><strong>不要对需要 autograd 的部分用原地替换（x[idx] &#x3D; …）</strong>，会破坏计算图，使用 <code>scatter</code>&#x2F;<code>scatter_add</code> 或构造新张量。</li><li><strong>索引返回的张量可能会改变梯度分配</strong>，只有被实际用到（参与 loss）的元素才会有梯度。</li><li><strong>尽量用 <code>gather</code> 实现批量按位置索引</strong>（可保持形状且直观），而不是复杂的循环。</li></ol><h1 id="17-一张速查小表（常用-API）"><a href="#17-一张速查小表（常用-API）" class="headerlink" title="17. 一张速查小表（常用 API）"></a>17. 一张速查小表（常用 API）</h1><ul><li>基本：<code>x[i]</code>, <code>x[:, j]</code>, <code>x[..., k]</code>, <code>x[start:stop:step]</code></li><li>维度：<code>unsqueeze</code>, <code>squeeze</code>, <code>transpose</code>, <code>permute</code>, <code>reshape</code>, <code>view</code>, <code>contiguous</code></li><li>选择&#x2F;花式：<code>index_select</code>, <code>take</code>, <code>gather</code>, <code>scatter</code>, <code>where</code>, <code>masked_select</code>, <code>masked_fill</code></li><li>布尔&#x2F;掩码：<code>x[mask]</code>, <code>torch.where</code>, <code>torch.nonzero</code></li><li>其他：<code>one_hot</code>, <code>topk</code>, <code>argmax/argmin</code>, <code>torch.arange</code>（用于构建 batch 索引）</li></ul><hr><p>如果你愿意，我可以：</p><ul><li>把这些示例做成一个可以直接运行的 Jupyter notebook（包含针对常见错误的测试用例），或者</li><li>根据你常用的框架（PyTorch &#x2F; TensorFlow &#x2F; NumPy）把示例改写成你习惯的 API，或</li><li>针对你当前代码中的索引问题（把你代码贴来），帮你找 bug 并修正。</li></ul><p>你想要哪种后续帮助？</p>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 张量 </tag>
            
            <tag> 运算 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Terchaer Forcing</title>
      <link href="/2024/09/01/teacher_forcing/"/>
      <url>/2024/09/01/teacher_forcing/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 架构 </tag>
            
            <tag> 模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer - 位置编码</title>
      <link href="/2024/04/21/Transfomer_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
      <url>/2024/04/21/Transfomer_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<blockquote><p>首先抛出一个问题, 为什么<code>Transformer</code> 要有位置编码?</p><p>– 对于任何一门语言, 单词在句子中的位置和顺序都极为重要, 语序正确, 才能表达出正确含义</p><blockquote><p>I <strong>do not</strong> like the story of the movie, but I <strong>do</strong> like the cast.<br>I <strong>do</strong> like the story of the movie, but I <strong>do not</strong> like the cast.</p></blockquote><p>如上述两个句子, 仅是改变<code>not</code>位置, 表达的意思截然相反</p><p>– <code>Transformer</code>抛弃了<code>RNN</code>和<code>CNN</code>作为序列学习的基本模型, 我们知道, 循环神经网络本身就是一种顺序结构, 天然包含了词在序列中的位置信息, 当抛弃循环神经网络结构, 完全采用<code>Attention</code>取而代之, 这些词序信息就会丢失, 模型就没有办法直到每个词在句子中的相对位置和绝对位置, 因此, 有必要把词序信息加到词向量上帮助模型学习这些信息, 位置编码(<code>Positional Encoding</code>)就是用来解决这个问题的办法</p></blockquote><h1 id="一-什么是位置编码"><a href="#一-什么是位置编码" class="headerlink" title="一. 什么是位置编码"></a>一. 什么是位置编码</h1><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/Transformer%2C_full_architecture.png"></p><p>在<code>transformer</code>的<code>encoder</code>和<code>decoder</code>中使用了<code>Positional Encoding</code>, 最终的输入就变为了:</p><p><em>input &#x3D; word_embedding + positional_encoding</em></p><p><strong>word_embedding</strong> : 词嵌入, 将<code>token</code>的维度从<code>vocab_size</code>映射到<code>d_model</code>(原论文中, <code>d_model</code>为512)</p><p>最终的输入通过词嵌入后的向量和位置编码矩阵相加得到, 所以<code>positional_encoding</code>也是<code>d_model</code>维度的向量</p><h1 id="二-位置编码构造方法"><a href="#二-位置编码构造方法" class="headerlink" title="二. 位置编码构造方法"></a>二. 位置编码构造方法</h1><h3 id="2-1-用增长数值标记位置"><a href="#2-1-用增长数值标记位置" class="headerlink" title="2.1 用增长数值标记位置"></a>2.1 用增长数值标记位置</h3><ul><li>第一个token标记1, 第二个token标记2… 以此类推</li></ul><p>这种方法存在一些问题:</p><ol><li>模型可能遇见比训练时所用序列更长的序列, 不利于模型泛化</li><li>模型无法理解数字的含义, 可能认为大数字比小数字权重更高, 而非顺序更靠后</li><li>无法表达相对位置信息</li></ol><h3 id="2-2-用-0-1-范围标记位置"><a href="#2-2-用-0-1-范围标记位置" class="headerlink" title="2.2 用[0, 1]范围标记位置"></a>2.2 用[0, 1]范围标记位置</h3><p>为解决上述问题, 将数值限制在[0, 1]区间内, 对其等分切割, 假设有3个token, 则位置信息为[0, 0.5, 1]</p><p>这样产生的问题是, 序列长度不同, 相对距离也不同, 模型可能认为这是单词语义发生了变化</p><h3 id="2-3-用二进制向量标记位置"><a href="#2-3-用二进制向量标记位置" class="headerlink" title="2.3 用二进制向量标记位置"></a>2.3 用二进制向量标记位置</h3><blockquote><p>由于位置信息最终会作用到<code>word_embedding</code>上, 比起用单一的数值, 更好的方式是使用和<code>word_embedding</code>同纬度的向量, 可以想到, 将数值转换成二进制形式</p></blockquote><p>假设<code>d_model</code> &#x3D; 3</p><p><img src="https://blog-vanh.oss-cn-hangzhou.aliyuncs.com/image/bi_pe.png"></p><p><code>d_model</code>一般比较大(512, 1024…), 基本可以把每个<code>token</code>的位置都编码出来</p><p>但这样也存在问题, 不同单词间的位置变化不连续, 难以推测相对位置信息</p><h3 id="2-4-用sin和cos函数交替标记位置"><a href="#2-4-用sin和cos函数交替标记位置" class="headerlink" title="2.4 用sin和cos函数交替标记位置"></a>2.4 用sin和cos函数交替标记位置</h3><blockquote><p>这是论文中提出的方法</p><p>三角函数有界且连续, 可以满足目前的需要</p></blockquote><p>$$<br>\text{PE}<em>{(pos, 2i)} &#x3D; \sin\left(\frac{pos}{10000^{2i&#x2F;d</em>{\text{model}}}}\right)<br>$$</p><p>$$<br>\text{PE}<em>{(pos, 2i+1)} &#x3D; \cos\left(\frac{pos}{10000^{2i&#x2F;d</em>{\text{model}}}}\right)<br>$$</p><p><strong>参数说明:</strong></p><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td><em>pos</em></td><td>当前位置（position index）</td></tr><tr><td><em>i</em></td><td>维度索引（dimension index）</td></tr><tr><td><em>d_model</em></td><td>模型维度（embedding size，例如512）</td></tr><tr><td>PE</td><td>位置编码矩阵（shape: <code>[seq_len, d_model]</code>）</td></tr></tbody></table><p>这里只给出计算公式, 数学推导网上优秀文章较多, 不再演示</p><h3 id="三-为什么可以直接将词向量和位置编码进行相加"><a href="#三-为什么可以直接将词向量和位置编码进行相加" class="headerlink" title="三. 为什么可以直接将词向量和位置编码进行相加"></a>三. 为什么可以直接将词向量和位置编码进行相加</h3><blockquote><p>这是一直困扰我的点, 仅是进行张量相加就可以达到添加位置信息的作用吗? 难道这样不会破坏词向量本身信息吗?</p><p>网上浏览了一些博主的文章和视频, 但是答案也并没那么明确, 有点大力出奇迹的意思</p></blockquote><h4 id="3-1、公式复习"><a href="#3-1、公式复习" class="headerlink" title="3.1、公式复习"></a>3.1、公式复习</h4><p>Transformer 的输入是：<br>$$<br>X &#x3D; E_{\text{word}} + E_{\text{pos}}<br>$$<br>其中：</p><ul><li>$E_{word}$：词向量（表示词语语义）</li><li>$E_{pos}$：位置编码（表示词语在句子中的位置）</li><li>维度相同（例如都为 $d_{model}&#x3D;512$）</li></ul><hr><h4 id="3-2、为什么相加就能引入位置信息？"><a href="#3-2、为什么相加就能引入位置信息？" class="headerlink" title="3.2、为什么相加就能引入位置信息？"></a>3.2、为什么相加就能引入位置信息？</h4><h5 id="1-向量加法在嵌入空间中表示“组合信息”"><a href="#1-向量加法在嵌入空间中表示“组合信息”" class="headerlink" title="1. 向量加法在嵌入空间中表示“组合信息”"></a>1. 向量加法在嵌入空间中表示“组合信息”</h5><p>在向量空间中，<strong>加法表示叠加不同的语义因素</strong>。</p><p>比如：</p><ul><li>“king” ≈ “man” + “royalty”</li><li>“Paris” - “France” + “Italy” ≈ “Rome”</li></ul><p>同理：</p><blockquote><p>把词向量和位置向量相加，就相当于在语义中叠加“第几个词”的信息。</p></blockquote><p>也就是说，<strong>新的向量同时携带了词义和位置两种特征</strong>。</p><hr><h5 id="2-相加不会“覆盖”信息，而是“偏移”语义空间"><a href="#2-相加不会“覆盖”信息，而是“偏移”语义空间" class="headerlink" title="2. 相加不会“覆盖”信息，而是“偏移”语义空间"></a>2. 相加不会“覆盖”信息，而是“偏移”语义空间</h5><p>我们可以把每个词向量看成在一个高维语义空间中的点。<br> 加上位置编码后，等价于把它 <strong>沿着“位置维度方向”平移了一点</strong>：<br>$$<br>new_embedding&#x3D;semantic_embedding+position_offset<br>$$<br>所以模型看到的不是“词义被破坏”，而是“相同词在不同位置处于略微不同的方向”。<br> 这使得 Transformer 能区分：</p><blockquote><p>“I love you”和“You love I” 的区别。</p></blockquote><hr><h5 id="3-注意力机制会自动学会利用这些位置差异"><a href="#3-注意力机制会自动学会利用这些位置差异" class="headerlink" title="3. 注意力机制会自动学会利用这些位置差异"></a>3. 注意力机制会自动学会利用这些位置差异</h5><p>在自注意力计算中，Q、K 向量都会从输入中线性变换得到：</p><p>$$<br>Q &#x3D; XW_Q, \quad K &#x3D; XW_K<br>$$<br>因此，位置信息通过 $E_{pos}$ 影响了 Query-Key 相似度。</p><p>当两个词位置不同、位置编码不同，它们的注意力权重分布也不同 → 模型能够学习“前后关系”。</p><hr><h4 id="3-3、为什么不会破坏词向量原始信息？"><a href="#3-3、为什么不会破坏词向量原始信息？" class="headerlink" title="3.3、为什么不会破坏词向量原始信息？"></a>3.3、为什么不会破坏词向量原始信息？</h4><h5 id="1-相加是线性可逆的（在一定程度上）"><a href="#1-相加是线性可逆的（在一定程度上）" class="headerlink" title="1. 相加是线性可逆的（在一定程度上）"></a>1. 相加是线性可逆的（在一定程度上）</h5><p>如果两个向量空间的分布相对独立，线性相加仍能被后续层（线性变换）分离：</p><p>$$<br>W(E_{word}+E_{pos})&#x3D;WE_{word}+WE_{pos}<br>$$<br><code>Transformer</code> 的多层线性结构可以在后续层中重新分辨“语义部分”和“位置信息部分”。</p><hr><h5 id="2-位置编码的幅度较小，不会淹没词向量"><a href="#2-位置编码的幅度较小，不会淹没词向量" class="headerlink" title="2. 位置编码的幅度较小，不会淹没词向量"></a>2. 位置编码的幅度较小，不会淹没词向量</h5><p>在实现时，位置编码通常被归一化或初始化为较小值（例如在 [-1,1] 范围内）。<br> 这样词向量的主导语义仍然保留，只是轻微偏移 → 注入位置信息。</p><hr><h5 id="3-高维空间中的“信息容量”巨大"><a href="#3-高维空间中的“信息容量”巨大" class="headerlink" title="3. 高维空间中的“信息容量”巨大"></a>3. 高维空间中的“信息容量”巨大</h5><p>在 512 维或更高的空间中，向量叠加后不会导致严重的信息重叠。<br> 就像 RGB 图像里混合红色和绿色还能区分出“黄色”一样，<br> 词义与位置信息叠加后仍可被网络区分和提取。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 位置编码 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>好用的工具</title>
      <link href="/2024/01/10/nicetools/"/>
      <url>/2024/01/10/nicetools/</url>
      
        <content type="html"><![CDATA[<h2 id="VPN"><a href="#VPN" class="headerlink" title="VPN"></a>VPN</h2><p><a href="https://www.efc123.com/shop">EFCloud</a> —— 跑路了，八嘎</p><p><a href="https://一元机场.ink/">一元机场</a> —— 非常稳定，几年了还在，就是节点很不稳定</p><p><a href="https://特价机场.com/">特价机场</a> —— 便宜好用</p><h2 id="ICON"><a href="#ICON" class="headerlink" title="ICON"></a>ICON</h2><p><a href="https://www.iconfont.cn/">阿里巴巴矢量图标库</a></p><p><a href="https://fontawesome.com/">FONT AWESOME</a></p><p><a href="https://yesicon.app/carbon/skill-level">Carbon</a></p><h3 id="封面图生成"><a href="#封面图生成" class="headerlink" title="封面图生成"></a>封面图生成</h3><p><a href="https://nav.rdonly.com/laboratory/bgimage/backimage.html">https://nav.rdonly.com/laboratory/bgimage/backimage.html</a></p><p><a href="https://cover.ruom.top/">https://cover.ruom.top/</a></p><h2 id="在线图片压缩"><a href="#在线图片压缩" class="headerlink" title="在线图片压缩"></a>在线图片压缩</h2><h3 id="1️⃣-TinyPNG"><a href="#1️⃣-TinyPNG" class="headerlink" title="1️⃣ TinyPNG"></a>1️⃣ <a href="https://tinypng.com/">TinyPNG</a></h3><ul><li>支持 PNG &#x2F; JPG &#x2F; WebP</li><li>一次可上传 20 张（单张 ≤ 5MB）</li><li>平均可减少 60–80% 体积</li><li>保留高画质，肉眼几乎无损</li></ul><p><strong>操作步骤：</strong></p><ol><li>打开网站</li><li>拖入图片</li><li>下载压缩后的版本</li></ol><p>👉 适合 <strong>博客封面、展示图、LOGO</strong> 等日常用途。</p><hr><h3 id="2️⃣-ILoveIMG"><a href="#2️⃣-ILoveIMG" class="headerlink" title="2️⃣ ILoveIMG"></a>2️⃣ <a href="https://www.iloveimg.com/compress-image">ILoveIMG</a></h3><ul><li>支持批量上传</li><li>同时压缩 JPG &#x2F; PNG &#x2F; GIF</li><li>提供在线编辑（裁剪、加水印等）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 效率 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
